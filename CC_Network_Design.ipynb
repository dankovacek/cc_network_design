{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pygrib\n",
    "\n",
    "from scipy import stats\n",
    "import scipy.special\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import nearest_points\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.tile_providers import Vendors, get_provider\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.models import Band, ColumnDataSource\n",
    "from bokeh.models import Label, LabelSet, Range1d\n",
    "from bokeh.palettes import Plasma4, Plasma6, Plasma10\n",
    "from bokeh.layouts import column, gridplot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import geoviews as gv\n",
    "import geoviews.feature as gf\n",
    "import geoviews.tile_sources as gvts\n",
    "\n",
    "from geoviews import opts\n",
    "\n",
    "from cartopy import crs\n",
    "\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "hv.extension('bokeh')\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Datasets\n",
    "\n",
    "Compare metadata from different datasets.\n",
    "\n",
    "\n",
    "       \n",
    "       'max_water_content', 'sand_frac',\n",
    "       'silt_frac', 'clay_frac', 'water_frac', 'organic_frac', 'other_frac',\n",
    "       'frac_forest', 'lai_max', 'lai_diff', 'gvf_max', 'gvf_diff',\n",
    "       'dom_land_cover_frac', 'dom_land_cover', 'root_depth_50',\n",
    "       'root_depth_99', 'p_mean', 'pet_mean', 'p_seasonality', 'frac_snow',\n",
    "       'aridity', 'high_prec_freq', 'high_prec_dur', 'high_prec_timing',\n",
    "       'low_prec_freq', 'low_prec_dur', 'low_prec_timing', 'geol_1st_class',\n",
    "       'glim_1st_class_frac', 'geol_2nd_class', 'glim_2nd_class_frac',\n",
    "       'carbonate_rocks_frac', \n",
    "\n",
    "| HYSETS Parameter Label | HYSETS | CAMELS | Notes |\n",
    "|---|---|---|---|\n",
    "| `Watershed_ID` | ✅ | ❌ | HYSETS-specific, simply numerical order | \n",
    "| `Source` | ✅ | ❌ | HYSETS is comprised of smaller datasets, source is the dataset origin. |\n",
    "| `Name` | ✅ | ✅ | Station \"Local\" Name, (`gauge_name` in CAMELS) |\n",
    "| `Official_ID` | ✅ | ✅ | ID specific to original datset, `gauge_id` in CAMELS |\n",
    "| `Centroid_Lat_deg_N` | ✅ | ❌ | CAMELS uses coords of **gauge location** (`gauge_lat`, `gauge_lon`) |\n",
    "| `Centroid_Lon_deg_E` | ✅ | ❌ | **could generate centroid coords from polygons...** |\n",
    "| `Drainage_Area_km2` | ✅ | ✅ | DA derived from basin polygons where available from origin agency. (CAMELS: `area_gages2`) |\n",
    "| `Drainage_Area_GSIM_km2` | ✅ | ❓ | Global Streamflow Indices and Metadata (GSIM) (CAMELS uses `area_geospa_fabric`  |\n",
    "| `Flag_GSIM_boundaries` | ✅ | ❌ | 1 (GSIM) 0 (official agencies) |\n",
    "| `Flag_Artificial_boundaries` | ✅ | ❌ | Bounds/DAs for stations < 50 $km^2$ (inaccurate delineation) |\n",
    "| `Elevation_m` | ✅ | ✅ | *mean* basin elevation (CAMELS: `elev_mean`) |\n",
    "| `Slope_deg` | ✅ |  | mean elevation difference between DEM \"tiles\" (CAMELS: `slope_mean`)  |\n",
    "| `Gravelius` | ✅ | ❌ | ratio of basin perimeter to a circle of the same area (large # indicates long catchment) |\n",
    "| `Perimeter` | ✅ | ❌ | can be calculated with polygons |\n",
    "| `Flag_Shape_Extraction` | ✅ | ❌ | ? |\n",
    "| `Aspect_deg` | ✅ | ❌ | \"main orientation\" \"where the average slope points towards\" |\n",
    "| `Flag_Terrain_Extraction` | ✅ | ❌ |  |\n",
    "| `Land_Use_Forest_frac` | ✅ |  | |\n",
    "| `Land_Use_Grass_frac` | ✅ |  | |\n",
    "| `Land_Use_Wetland_frac` | ✅ |  | |\n",
    "| `Land_Use_Water_frac` | ✅ | ✅ | |\n",
    "| `Land_Use_Urban_frac` | ✅ |  | |\n",
    "| `Land_Use_Shrubs_fra` | ✅ |  | |\n",
    "| `Land_Use_Crops_frac` | ✅ |  | |\n",
    "| `Land_Use_Snow_Ice_frac` | ✅ |  | |\n",
    "| `Flag_Land_Use_Extraction` | ✅ |  | |\n",
    "| `Permeability_logk_m2` | ✅ |  | subsurface permeability below soil horizon (arithmetic mean) (CAMELS: `geol_permeability`)|\n",
    "| `Porosity_frac` | ✅ |  | soil porosity $\\frac{V_v}{V_T}$ (geometric mean) (CAMELS: `geol_porostiy` |\n",
    "| `Flag_Subsoil_Extraction` | ✅ |  | |\n",
    "| `q_mean` | ❌ | ✅ | can be calculated for HYSETS |\n",
    "| `runoff_ratio` | ❌ | ✅ | can be calculated for HYSETS |\n",
    "| `slope_fdc` | ❌ | ✅ | ? can be calculated for HYSETS? |\n",
    "| `baseflow_index` | ❌ | ✅ |? can be calculated for HYSETS? |\n",
    "| `stream_elas` | ❌ | ✅ | ? can be calculated for HYSETS |\n",
    "| `q5` | ❌ | ✅ | can be calculated for HYSETS |\n",
    "| `q95` | ❌ | ✅ | can be calculated for HYSETS |\n",
    "| `high_q_freq` | ❌ | ✅ | can be calculated for HYSETS |\n",
    "| `high_q_dur` | ❌ | ✅ | can be calculated for HYSETS |\n",
    "| `low_q_freq` | ❌ | ✅ | can be calculated for HYSETS |\n",
    "| `low_q_dur` | ❌ | ✅ | can be calculated for HYSETS |\n",
    "| `zero_q_freq` | ❌ | ✅ | can be calculated for HYSETS |\n",
    "| `hfd_mean` | ❌ | ✅ | ? can be calculated for HYSETS |\n",
    "| `huc_02` | ❌ | ✅ | ? |\n",
    "| `soil_depth_pelletier` | ❌ | ✅ | ? |\n",
    "| `soil_depth_statsgo` | ❌ | ✅ | ? |\n",
    "| `taxmax/tasmin` | ✅ | ? | Daily Maximum/Minimum Near-Surface Air Temperature |\n",
    "| `tasmin` | ❌ | ✅ | ? |\n",
    "| `` | ❌ | ✅ | ? |\n",
    "\n",
    "## CAMELS Classes\n",
    "\n",
    "CAMELS groups catchment attributes in six classes: topography, climate, streamflow, land cover, soil, and geology.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hysets\n",
    "hysets_df = pd.read_csv('data/HYSETS_watershed_properties.txt', sep=';', dtype={'Official_ID': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hysets_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of identifying information to facilitate\n",
    "# selection of specific watersheds\n",
    "hysets_dict = hysets_df[['Watershed_ID', 'Official_ID', 'Drainage_Area_km2', 'Name']].set_index('Official_ID').to_dict(orient='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_official_IDs = list(hysets_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_geodf(tp1, tp2, lat_tag, lon_tag, df):\n",
    "    df_obs = pd.DataFrame({'latitude': df[lat_tag].to_numpy(), \n",
    "                       'longitude': df[lon_tag].to_numpy(), \n",
    "                       tp1: df[tp1],\n",
    "                       tp2: df[tp2]\n",
    "                    })\n",
    "\n",
    "#     gdf = gpd.GeoDataFrame(df_obs, geometry=gpd.points_from_xy(df_obs['longitude'], df_obs['latitude']), crs='epsg:3857')\n",
    "    return gv.Points(df_obs, ['longitude', 'latitude'], [tp1], label='DA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp1 = 'Drainage_Area_km2'\n",
    "tp2 = 'Official_ID'\n",
    "cm_lat = 'Centroid_Lat_deg_N'\n",
    "cm_lon = 'Centroid_Lon_deg_E'\n",
    "n_watersheds = len(hysets_df)\n",
    "\n",
    "hysets_pts = create_geodf(tp1, tp2, cm_lat, cm_lon, hysets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(gvts.EsriNatGeo *\n",
    " hysets_pts.opts(\n",
    "     title=f'HYSETS Watershed Dataset ({n_watersheds} watersheds)',\n",
    "     logz=True,\n",
    "     size=2, #+ gv.dim(target_param)*2, \n",
    "     fill_color=tp1, \n",
    "     line_color=None,\n",
    "     cmap='Plasma',\n",
    "     alpha=0.5,\n",
    "     colorbar=True, clabel='km²',\n",
    "     width=700, \n",
    "     height=450, \n",
    "     global_extent=False, \n",
    "     tools=['hover'], \n",
    "     show_legend=False))\n",
    "#      show_legend=False) *\n",
    "#  gv.Labels(pts).opts(\n",
    "#     text_font_size='8pt', text_color='black')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_hyd = pd.read_csv('data/camels_attributes_v2.0/camels_hydro.txt', sep=';', dtype={'gauge_id': str})\n",
    "cm_names = pd.read_csv('data/camels_attributes_v2.0/camels_name.txt', sep=';', dtype={'gauge_id': str})\n",
    "cm_topo = pd.read_csv('data/camels_attributes_v2.0/camels_topo.txt', sep=';', dtype={'gauge_id': str})\n",
    "cm_soil = pd.read_csv('data/camels_attributes_v2.0/camels_soil.txt', sep=';', dtype={'gauge_id': str})\n",
    "cm_vege = pd.read_csv('data/camels_attributes_v2.0/camels_vege.txt', sep=';', dtype={'gauge_id': str})\n",
    "cm_clim = pd.read_csv('data/camels_attributes_v2.0/camels_clim.txt', sep=';', dtype={'gauge_id': str})\n",
    "cm_geol = pd.read_csv('data/camels_attributes_v2.0/camels_geol.txt', sep=';', dtype={'gauge_id': str})\n",
    "\n",
    "dfs = [cm_hyd, cm_names, cm_topo, cm_soil, cm_vege, cm_clim, cm_geol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map names to the cm_hyd file\n",
    "from functools import reduce\n",
    "\n",
    "camels_df = reduce(lambda left, right: pd.merge(left, right, on='gauge_id'), dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camels_df.head()\n",
    "print(len(camels_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check overlap between CAMELS and HYSETS\n",
    "cm = camels_df[['gauge_id', 'gauge_name']].copy()\n",
    "hs = hysets_df[['Official_ID', 'Name']].copy()\n",
    "cm.set_index('gauge_id', inplace=True)\n",
    "hs.set_index('Official_ID', inplace=True)\n",
    "\n",
    "overlap = pd.concat([cm, hs], join='inner', axis=1)\n",
    "print(f'There are {len(overlap)} stations in common between CAMELS and HYSETS.')\n",
    "overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp1 = 'area_gages2'\n",
    "tp2 = 'area_geospa_fabric'\n",
    "cm_lat = 'gauge_lat'\n",
    "cm_lon = 'gauge_lon'\n",
    "n_watersheds = len(camels_df)\n",
    "\n",
    "camels_pts = create_geodf(tp1, tp2, cm_lat, cm_lon, camels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(gvts.EsriNatGeo *\n",
    " camels_pts.opts(\n",
    "     title=f'HYSETS Watershed Dataset ({n_watersheds} watersheds)',\n",
    "     logz=True,\n",
    "     size=4, #+ gv.dim(target_param)*2, \n",
    "     fill_color=tp1, \n",
    "     line_color=None,\n",
    "     cmap='Plasma',\n",
    "     alpha=0.5,\n",
    "     colorbar=True, clabel='km²',\n",
    "     width=700, \n",
    "     height=450, \n",
    "     global_extent=False, \n",
    "     tools=['hover'], \n",
    "     show_legend=False))\n",
    "#      show_legend=False) *\n",
    "#  gv.Labels(pts).opts(\n",
    "#     text_font_size='8pt', text_color='black'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import HYSETS Data\n",
    "\n",
    "Below is a test demo to load the HYSETS database and extract the (non-null) timeseries data for a specific HYDAT timeseries by the `Official_ID` parameter, which is the ID used by the governing organization that manages the dataset.\n",
    "\n",
    "### NOTE: When using `.sel()` on xarray, the watershedID in the HYSETS dict starts at 1, while the xarray dataset is zero indexed.\n",
    "\n",
    "As a result, **subtract 1 from the watershedID when using `.sel()`, i.e.:\n",
    "\n",
    ">`data = ds.sel(watershed=ws['Watershed_ID']-1, drop=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd())))\n",
    "# DB_DIR = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "DB_DIR = os.path.join(BASE_DIR, 'hysets_db/')\n",
    "\n",
    "hysets_filename = 'HYSETS_2020_NRCAN.nc'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(DB_DIR + hysets_filename)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 08MH147 is Stave River above Stave Lake\n",
    "# ws = hysets_dict['08MH147']\n",
    "# print(ws)\n",
    "# print(ws['Watershed_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_streamflow_series(stn):\n",
    "    ws = hysets_dict[stn]\n",
    "    df = ds.sel(watershed=ws['Watershed_ID']-1, drop=True).to_dataframe()\n",
    "\n",
    "    df = df[['discharge']].dropna()\n",
    "    wsid = ws['Watershed_ID']\n",
    "\n",
    "    df.to_csv(f'/media/ danbot/T7 Touch/hysets_series/{stn}.csv')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "with Pool() as pool:\n",
    "    pool.map(extract_streamflow_series, all_official_IDs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the main dataframe for the watershed of interest\n",
    "ds_filtered = ds.sel(watershed=ws['Watershed_ID']-1, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds_filtered.to_dataframe()\n",
    "df = df[~df['discharge'].isnull()]\n",
    "\n",
    "df.index = pd.to_datetime(df.index)\n",
    "n_years = len(set(df.index.year))\n",
    "\n",
    "print(df['discharge'].max(), df['discharge'].min())\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compression Analysis\n",
    "\n",
    "Quantize the entire dataset with n [equiprobable bins](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binned_statistic.html).  This results in a uniform distribution, and no effect is introduced by tuning distribution parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uniform_noise(n):\n",
    "    noise = np.random.uniform(-1, 1, n)\n",
    "    return noise / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the set of all repeated variables and add uniform noise\n",
    "# to facilitate equiprobable bin creation.\n",
    "import collections\n",
    "\n",
    "repeated_vals = [item for item, count in collections.Counter(df['discharge'].to_numpy()).items() if count > 1]\n",
    "\n",
    "for repeated_val in repeated_vals:\n",
    "    n_repeats = len(df.loc[df['discharge'] == repeated_val, 'discharge'])\n",
    "    df.loc[df['discharge'] == repeated_val, 'discharge'] += generate_uniform_noise(n_repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "# Quantize the daily flow series\n",
    "# set the bounds at 0 and the 99th percentile flow\n",
    "# stat, edges, bnum = st.binned_statistic(df['discharge'], values=None, statistic='count', bins=8, range=(0, np.percentile(df['discharge'], 99)))\n",
    "def derive_equiprobable_bin_edges(n_bins, df):\n",
    "    n_obs = len(df)\n",
    "    probs = np.linspace(0.0,1, n_bins+1)\n",
    "    bin_edges = st.mstats.mquantiles(df['discharge'], prob=probs)\n",
    "    # print(hist)\n",
    "#     print('    bin edges:')\n",
    "#     print(f'        {bin_edges.round(1)}')\n",
    "    return bin_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = derive_equiprobable_bin_edges(8, df)\n",
    "df['bin_no'] = np.digitize(df['discharge'], bin_edges)\n",
    "hist, edges = np.histogram(df['discharge'], bin_edges, density=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(title, hist, edges, x=None, pdf=None, cdf=None):\n",
    "    p = figure(title=title, background_fill_color=\"#fafafa\",\n",
    "               width=600, height=450,\n",
    "#               y_range=(1505, 1510),\n",
    "              )\n",
    "    p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:],\n",
    "           fill_color=\"navy\", line_color=\"white\", alpha=0.5, \n",
    "          legend_label='Test')\n",
    "#     p.line(x, pdf, line_color=\"#ff8888\", line_width=4, alpha=0.7, legend_label=\"PDF\")\n",
    "#     p.line(x, cdf, line_color=\"orange\", line_width=2, alpha=0.7, legend_label=\"CDF\")\n",
    "\n",
    "#     p.y_range.start = 0\n",
    "    p.legend.location = \"top_right\"\n",
    "    p.legend.background_fill_color = \"#fefefe\"\n",
    "    p.xaxis.axis_label = 'Q [cms]'\n",
    "    p.yaxis.axis_label = 'Pr(x)'\n",
    "    p.grid.grid_line_color=\"white\"\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 8\n",
    "p = make_plot(f'Equiprobable Binning ({n_bins} bins)', hist, edges, )\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array of all combinations of periods \n",
    "# this will be used as input to filter the dataframe\n",
    "# for sub-periods \n",
    "import random\n",
    "import itertools\n",
    "\n",
    "def format_input_data(df):\n",
    "    years = list(set(df.index.year))\n",
    "    \n",
    "    all_periods = []\n",
    "    len_tracker = {}\n",
    "\n",
    "    for period_len in range(1, 5):\n",
    "        samples = list(itertools.combinations(years, period_len))\n",
    "        all_periods += samples\n",
    "#         print(f'There are {len(samples)} samples for a period length of {period_len} years.')\n",
    "        len_tracker[period_len] = len(samples)\n",
    "    return all_periods, len_tracker\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(probs, bins, base=2):\n",
    "    \"\"\" Computes entropy of label distribution. \n",
    "    By default, \n",
    "        -uses base 2.\n",
    "        -uses 8 bins.\n",
    "        \n",
    "    Returns Entropy of a sequence (in bits per sample)\n",
    "    \"\"\"\n",
    "    ent = 10E-20\n",
    "    # Compute entropy from probability (counts)\n",
    "    for p in probs:\n",
    "        if p != 0:\n",
    "            # convert to \n",
    "            p = p / sum(probs)\n",
    "            ent -= p * math.log(p, base)\n",
    "            \n",
    "    return ent \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate entropy\n",
    "h_complete = round(calculate_entropy(hist, n_bins), 3)\n",
    "\n",
    "print(f'The entropy of the entire distribution is {h_complete} bits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Entropy over (all) periods of varying length\n",
    "\n",
    "Above, bin edges were determined for a full dataset based on equiprobable bins.  For periods from 1 to len(record) years, calculate the entropy of subsets of data using the same bin edges, and also collect the bin edges for subsets to develop a distribution of both bin edge locations and entropy for different sub-period lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_const_edges(input_data):\n",
    "    sample_len = len(input_data[0])\n",
    "    data = input_data[-1]\n",
    "    n_bins = input_data[2]\n",
    "    years_in_subsample = input_data[0]\n",
    "    sample_data = data[data.index.year.isin(years_in_subsample)]\n",
    "    hist, edges = np.histogram(sample_data['bin_no'], density=False, bins=n_bins)\n",
    "    sample_h = calculate_entropy(hist, bins=n_bins)\n",
    "    return (sample_len, sample_h)\n",
    "\n",
    "def calculate_entropy_distributions(df, bin_edges, n_bins, all_periods):\n",
    "    input_array = [(y, bin_edges, n_bins, df) for y in all_periods]\n",
    "    t0 = time.time()\n",
    "    pool = Pool()\n",
    "    result_const_bin_edges = pool.map(entropy_const_edges, input_array)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    t1 = time.time()\n",
    "    t_tot = t1 - t0\n",
    "    print(f'for {n_bins} bins, time to calculate all scenarios: {t_tot:.1f}s')\n",
    "    return result_const_bin_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_output_to_dict(results):\n",
    "    sample_lens = list(set([e[0] for e in results]))\n",
    "    results_dict = {l: [] for l in sample_lens}\n",
    "    for r in results:\n",
    "        results_dict[r[0]].append(r[1])\n",
    "        \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_mean_max(results, n_bins, len_tracker):\n",
    "    \n",
    "    out_mean, out_max, out_min = [], [], []\n",
    "    n_samples = []\n",
    "    max_entropy = np.log2(n_bins)\n",
    "    for p in results.keys():\n",
    "        out_mean.append(np.mean(results[p]) / max_entropy)\n",
    "        out_max.append(np.percentile(results[p], 95) / max_entropy)\n",
    "        out_min.append(np.percentile(results[p], 5) / max_entropy)\n",
    "        n_samples.append(f'n={len_tracker[p]}')\n",
    "        \n",
    "    print(f'    mean entropy: ')\n",
    "    print(f'         {[e.round(2) for e in out_mean]}')\n",
    "        \n",
    "    return pd.DataFrame({'sample_len': results.keys(),\n",
    "                        'mean': out_mean,\n",
    "                        'max': out_max,\n",
    "                        'min': out_min,\n",
    "                         'n_samples': n_samples,\n",
    "                         'max_entropy': [1 for e in results.keys()]\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_main(df, n_bins):\n",
    "    print(f'Starting entropy calculation for {n_bins} bins')\n",
    "    # set the bin edges for the total sample based on equiprobable bins\n",
    "    bin_edges = derive_equiprobable_bin_edges(n_bins, df)\n",
    "    # prepare input data of subsamples\n",
    "    all_periods, len_tracker = format_input_data(df)\n",
    "    # calculate entropy for all subsamples\n",
    "    results_raw = calculate_entropy_distributions(df, bin_edges, n_bins, all_periods)\n",
    "    results_dict_const_bin = convert_output_to_dict(results_raw)\n",
    "#     len_tracker_dict = {k: len([e for e in all_periods if len(e) == k]) for k in range(1,5)}\n",
    "    results_df = min_mean_max(results_dict_const_bin, n_bins, len_tracker)\n",
    "    print('')\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_bin_results = entropy_main(df, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = ColumnDataSource(const_bin_results)\n",
    "\n",
    "fig = figure(title=f\"Compressibility Distribution by Sample Length (8 bins)\",\n",
    "           width=600, height=450, x_range=(0.8, 4.5), y_range=(0.2, 1.1))\n",
    "\n",
    "fig.line(const_bin_results['sample_len'], const_bin_results['mean'], line_color=\"green\", line_width=3, \n",
    "         alpha=0.7, legend_label=\"Mean\", line_dash='dashed')\n",
    "\n",
    "fig.varea(x='sample_len', y1='min', y2='max', source=source, level='underlay',\n",
    "           fill_alpha=0.5, fill_color='dodgerblue', legend_label='2σ interval')\n",
    "\n",
    "fig.line([1, 4], [1, 1], color='firebrick', alpha=0.8, line_width=3, \n",
    "         legend_label='Maximum Entropy', line_dash='dashed')\n",
    "\n",
    "# add labels for sample size at each subset length\n",
    "labels = LabelSet(x='sample_len', y='max_entropy', text='n_samples', level='glyph',\n",
    "              x_offset=-15, y_offset=5, source=source, render_mode='canvas')\n",
    "\n",
    "fig.add_layout(labels)\n",
    "\n",
    "fig.legend.location = \"center_right\"\n",
    "fig.xaxis.axis_label = 'Subset length (n years)'\n",
    "fig.yaxis.axis_label = 'Temporal Compression [bps]'\n",
    "\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_comparison = {}\n",
    "for n in [6, 8, 10, 12, 14, 16]:\n",
    "    bin_comparison[n] = entropy_main(df, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = ColumnDataSource(const_bin_results)\n",
    "\n",
    "fig = figure(title=f\"Compressibility Distribution by Sample Length as a function of Number of Bins\",\n",
    "           width=600, height=450, x_range=(0.5, 4.5), y_range=(0., 1.02))\n",
    "\n",
    "i = 0\n",
    "for k in bin_comparison.keys():\n",
    "    x = bin_comparison[k]['sample_len']\n",
    "    y = bin_comparison[k]['mean']\n",
    "    fig.line(x, y, legend_label=f'{k} bins', color=Plasma6[i], line_width=3)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "\n",
    "fig.line([1, 4], [1, 1], color='firebrick', alpha=0.8, line_width=3, \n",
    "         legend_label='Maximum Entropy', line_dash='dashed')\n",
    "\n",
    "\n",
    "fig.legend.location = \"bottom_right\"\n",
    "fig.xaxis.axis_label = 'Subset length (n years)'\n",
    "fig.yaxis.axis_label = 'Temporal Compression Ratio [-]'\n",
    "\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bin Edges\n",
    "\n",
    "How sensitive are the bin edge definitions?  Instead of using constant bin edges defined by the entire record as above, determine the distribution of bin edge locations based on all the subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_edge_sensitivity(input_data):\n",
    "    data = input_data[0]\n",
    "    sample_periods = input_data[2]\n",
    "    sample_data = data[data.index.year.isin(sample_periods)]\n",
    "    n_bins = input_data[1]\n",
    "    sample_len = len(sample_periods)\n",
    "    bin_edges = derive_equiprobable_bin_edges(n_bins, sample_data)\n",
    "    return (sample_len, bin_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "input_data_array = []\n",
    "\n",
    "all_periods, len_tracker = format_input_data(df)\n",
    "input_array = [(df, n_bins, p) for p in all_periods]\n",
    "t0 = time.time()\n",
    "pool = Pool(12)\n",
    "results_var_bin_edges = pool.map(bin_edge_sensitivity, input_array)\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "t1 = time.time()\n",
    "t_tot = t1 - t0\n",
    "print(f'for {n_bins} bins, time to calculate {len(all_periods)} scenarios: {t_tot:.1f}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_dict = {}\n",
    "for r in results_var_bin_edges:\n",
    "    sample_len = r[0]\n",
    "    if sample_len not in edge_dict.keys():\n",
    "        edge_dict[sample_len] = [r[1]]\n",
    "    else:\n",
    "        edge_dict[sample_len].append(r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_sensitivity_dict = {}\n",
    "for k in edge_dict.keys():\n",
    "    edge_df = pd.DataFrame(edge_dict[k])\n",
    "    edge_df.loc['Mean', :] = edge_df.mean(axis=0)\n",
    "    edge_df.loc['5pct', :] = edge_df.quantile(.05, axis=0)\n",
    "    edge_df.loc['95pct', :] = edge_df.quantile(.95, axis=0)\n",
    "    edge_sensitivity_dict[k] = edge_df.loc[['Mean', '5pct', '95pct'], :].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig_row = []\n",
    "for k in edge_dict.keys():\n",
    "    fig = figure(title=f\"Bin Edge Sensitivity ({k} years)\",\n",
    "           width=700, height=450)\n",
    "    i = 0\n",
    "    for r in edge_sensitivity_dict[k].index:\n",
    "        # center of the vertical bar\n",
    "        x = edge_sensitivity_dict[k].loc[r, ['Mean']]\n",
    "        # width of the vertical bar\n",
    "        w = edge_sensitivity_dict[k].loc[r, '95pct'] - edge_sensitivity_dict[k].loc[r, '5pct']\n",
    "        fig.line([x, x], [0, 0.5], color=Plasma10[i], line_width=3, line_dash='dashed')\n",
    "        \n",
    "        fig.vbar(x=x, top=0.5, bottom=0, width=w, level='underlay',\n",
    "           fill_alpha=0.5, fill_color=Plasma10[i],\n",
    "                line_width=0)\n",
    "        i += 1\n",
    "    \n",
    "#     fig.legend.location = \"top_left\"\n",
    "    fig.xaxis.axis_label = 'Q [cms]'\n",
    "    fig.yaxis.axis_label = 'P(X)'\n",
    "    fig_row.append(fig)\n",
    "    \n",
    "\n",
    "\n",
    "grid = gridplot([fig_row[:2], fig_row[2:]], plot_width=400, plot_height=350)\n",
    "    \n",
    "show(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(title, hist, edges, x, pdf, cdf):\n",
    "    p = figure(title=title, tools='', background_fill_color=\"#fafafa\")\n",
    "    p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:],\n",
    "           fill_color=\"navy\", line_color=\"white\", alpha=0.5)\n",
    "    p.line(x, pdf, line_color=\"#ff8888\", line_width=4, alpha=0.7, legend_label=\"PDF\")\n",
    "    p.line(x, cdf, line_color=\"orange\", line_width=2, alpha=0.7, legend_label=\"CDF\")\n",
    "\n",
    "    p.y_range.start = 0\n",
    "    p.legend.location = \"center_right\"\n",
    "    p.legend.background_fill_color = \"#fefefe\"\n",
    "    p.xaxis.axis_label = 'x'\n",
    "    p.yaxis.axis_label = 'Pr(x)'\n",
    "    p.grid.grid_line_color=\"white\"\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the bin edges as distributions\n",
    "fig = figure(title=f\"Bin Edge Sensitivity ({k} years)\",\n",
    "       width=700, height=450)\n",
    "\n",
    "sub_bins = [5, 10, 13, 15]\n",
    "b = 0\n",
    "for l in edge_dict.keys():\n",
    "    \n",
    "    edge_df = pd.DataFrame(edge_dict[l])\n",
    "    # width of the vertical bar\n",
    "#     print(f'for sample length {l} there are {len(edge_df)} samples')\n",
    "    y = l\n",
    "    i = 0\n",
    "#     print(f'sample length: {l}')\n",
    "    for c in edge_df.columns:\n",
    "#         print(edge_df[c])\n",
    "        n = int(10E-4 * len(edge_df) + 5)\n",
    "        num_sub_bins = sub_bins[b]\n",
    "#         print(f'there should be {num_sub_bins} sub bins for {l} sample len')\n",
    "        hist, edges = np.histogram(edge_df[c], density=True)#, bins=sub_bins[b])\n",
    "        fig.quad(top=hist + l, bottom=l, left=edges[:-1], right=edges[1:],\n",
    "            fill_color=Plasma10[i], line_color=None, alpha=0.7)\n",
    "        i += 1\n",
    "    b += 1\n",
    "\n",
    "fig.xaxis.axis_label = 'Distribution of Bin Edges (Q [cms])'\n",
    "fig.yaxis.axis_label = 'Sample Length (Years)'\n",
    "\n",
    "print('__')\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
