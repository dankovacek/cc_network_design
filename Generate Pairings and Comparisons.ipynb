{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paired Watershed Characteristics\n",
    "\n",
    "Develop framework to compare pairs of daily flow series from basins in the WSC database.  \n",
    "\n",
    "## Method:\n",
    "\n",
    "1. Generate a list of valid pairs of stations. A valid pair is one where:\n",
    "    * basin geometry exists for both stations\n",
    "    * there is a minimum N years of concurrent data between the two stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basin characteristics\n",
    "WSC_db_folder = '/media/danbot/T7 Touch/hydat_db/'\n",
    "metadata_fn = 'WSC_Stations_Master.csv'\n",
    "hysets_folder = '/media/danbot/T7 Touch/hysets_series/'\n",
    "\n",
    "df = pd.read_csv(WSC_db_folder + metadata_fn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_years_record'] = df['Year To'] - df['Year From']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for stations in BC and Alberta\n",
    "df = df[df['Province'].isin(['BC', 'AB'])]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stn_pairs_list = list(combinations(df['Station Number'].to_numpy(), 2))\n",
    "print(len(df))\n",
    "print(len(stn_pairs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hysets_df = pd.read_csv('data/HYSETS_watershed_properties.txt', sep=';', dtype={'Official_ID': str})\n",
    "hysets_df = hysets_df[hysets_df['Source'] == 'HYDAT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a centroid shapely Point\n",
    "hysets_df['centroid_geom'] = hysets_df.apply(lambda xy: Point((xy['Centroid_Lon_deg_E'], xy['Centroid_Lat_deg_N'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hysets_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of identifying information to facilitate\n",
    "# selection of specific watersheds\n",
    "basin_metadata = ['Watershed_ID', 'Official_ID', 'Name']\n",
    "\n",
    "basin_centroid_geom = ['centroid_geom']\n",
    "\n",
    "basin_characteristics_cols = ['Drainage_Area_km2', 'Centroid_Lat_deg_N',\n",
    "                              'Centroid_Lon_deg_E', 'Elevation_m', 'Gravelius', \n",
    "                              'Aspect_deg', 'Slope_deg', 'Land_Use_Forest_frac',\n",
    "                              'Land_Use_Grass_frac', 'Land_Use_Wetland_frac', \n",
    "                              'Land_Use_Water_frac', 'Land_Use_Urban_frac', \n",
    "                              'Land_Use_Shrubs_frac', 'Land_Use_Crops_frac',\n",
    "                              'Land_Use_Snow_Ice_frac', 'Permeability_logk_m2', \n",
    "                              'Porosity_frac']\n",
    "\n",
    "hysets_dict = hysets_df[basin_metadata + basin_centroid_geom + basin_characteristics_cols].set_index('Official_ID').to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hysets_stns = list(hysets_dict.keys())\n",
    "n_hydat_stns = len(hysets_stns)\n",
    "print(f'There are {n_hydat_stns} HYDAT station records in the HYSETS database.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pair_df = pd.DataFrame(stn_pairs_list, columns=['b1', 'b2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_pair_in_hysets(pair):\n",
    "    return (pair[0] in hysets_stns) & (pair[1] in hysets_stns)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool()\n",
    "t0 = time.time()\n",
    "pair_df['pair_in_hysets'] = pool.map(check_if_pair_in_hysets, stn_pairs_list)\n",
    "pool.close()\n",
    "pool.join()\n",
    "t1 = time.time()\n",
    "print(f't for {len(stn_pairs_list)} results: {t1-t0:.1f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'len before filter = {len(pair_df)}')\n",
    "pair_df = pair_df[pair_df['pair_in_hysets']]\n",
    "print(f'len after filter = {len(pair_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pair_properties(row):\n",
    "    for c in basin_characteristics_cols:\n",
    "        p1 = hysets_dict[row['b1']][c]\n",
    "        p2 = hysets_dict[row['b2']][c]\n",
    "    if ~np.isnan(p1) & ~np.isnan(p2):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pair_df['char_check'] = pair_df.apply(lambda row: check_pair_properties(row), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_characteristics = pair_df[~pair_df['char_check']].count()\n",
    "print(f'{len(missing_characteristics)} basins have missing characteristics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out pairs missing basin characteristics\n",
    "pair_df = pair_df[pair_df['char_check']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_streamflow_series(stn):\n",
    "#     ws = hysets_dict[stn]\n",
    "#     df = ds.sel(watershed=ws['Watershed_ID']-1, drop=True).to_dataframe()\n",
    "    df = pd.read_csv(f'{hysets_folder}{stn}.csv', index_col=['time'])\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_actual_concurrence_len(pair):\n",
    "    df1 = extract_streamflow_series(pair[0])\n",
    "    df1.rename(mapper={'discharge': f'{pair[0]}'}, inplace=True, axis=1)\n",
    "    \n",
    "    df2 = extract_streamflow_series(pair[1])\n",
    "    df2.rename(mapper={'discharge': f'{pair[1]}'}, inplace=True, axis=1)\n",
    "    concurrent_df = pd.concat([df1, df2], join='inner', axis=1)\n",
    "    return len(concurrent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for pairs that have minimum 50 years of concurrent data\n",
    "t0 = time.time()\n",
    "pool = Pool()\n",
    "\n",
    "# pair_df['concurrence_check'] = pool.map(check_actual_concurrence_len, pair_df[['b1', 'b2']].to_numpy()[:10])\n",
    "pair_df['concurrent_days'] = pool.map(check_actual_concurrence_len, \n",
    "               pair_df[['b1', 'b2']].to_numpy())\n",
    "pool.close()\n",
    "pool.join()\n",
    "t1 = time.time()\n",
    "print(f'Time to calculate concurrent period lengths: {t1 - t0:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pair_df.head())\n",
    "print(len(pair_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_df = pair_df[pair_df['concurrent_days'] > 365]\n",
    "print(f'{len(pair_df)} basin pairs meet the concurrence length criteria.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the list of unique pairs to disk so you \n",
    "# don't have to go through that process again\n",
    "pair_df.to_pickle('results/filtered_pairs_all_concurrent_lengths.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_pairs = pair_df[['b1', 'b2']].to_numpy()\n",
    "filtered_pairs = [tuple(e) for e in filtered_pairs]\n",
    "unique_concurrent = list(set(filtered_pairs))\n",
    "print(unique_concurrent[:5])\n",
    "print(len(unique_concurrent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Load all Saved Results\n",
    "\n",
    "Continue the distance metric calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_pickle('results/filtered_pairs_all_concurrent_lengths.csv')\n",
    "all_df = all_df[['b1', 'b2', 'concurrent_days']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run similarity operation on filtered pairs\n",
    "\n",
    "1. Calculate a 'similarity' metric based on concurrent data.\n",
    "2. Retrieve basin characteristics from the hysets basin characteristics file.\n",
    "3. Calculate differences in basin elevation, gravelius, drainage area, and distance between basin centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_diff(pair, param):\n",
    "    return hysets_dict[pair[0]][param] - hysets_dict[pair[1]][param]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(pair):\n",
    "    foo = hysets_df[hysets_df['Official_ID'].isin(pair)]\n",
    "    hdf = gpd.GeoDataFrame(foo, geometry=foo['centroid_geom'], crs='EPSG:4326')\n",
    "    hdf = hdf.to_crs(3005)\n",
    "    hdf.reset_index(inplace=True)\n",
    "    return hdf.loc[0, 'geometry'].distance(hdf.loc[1, 'geometry']) / 1000\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely.geometry as geom\n",
    "\n",
    "def create_line(row):\n",
    "    return geom.LineString([hysets_dict[row['b1']]['centroid_geom'], hysets_dict[row['b2']]['centroid_geom']])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def set_centroid_geom()\n",
    "\n",
    "# all_df['centroids'] = all_df.apply(row: Point(xy) for xy in )\n",
    "geometry = gpd.GeoDataFrame({'geometry': all_df.apply(lambda row: create_line(row), axis=1)}, crs='EPSG:4326')\n",
    "\n",
    "geometry = geometry.to_crs(3005)\n",
    "geometry['centroid_distance_km'] = geometry.length / 1000  # convert to km\n",
    "geometry.head()\n",
    "# all_df.head()\n",
    "\n",
    "# all_df['b1'].apply(lambda e: hysets_dict[e]['centroid_geom']) \n",
    "# all_df['b2_centroid_geom'] = all_df['b2'].apply(lambda e: hysets_dict[e]['centroid_geom']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['distance_btwn_centroids_km'] = geometry['centroid_distance_km']\n",
    "\n",
    "all_df['pair_midpoint'] = geometry['geometry'].interpolate(0.5, normalized=False)\n",
    "\n",
    "all_df.head()\n",
    "# print(hysets_dict['05AA006'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair_df['normed_distance'] = (pair_df['PC_distance'] - pair_df['PC_distance'].min()) / (pair_df['PC_distance'].max() - pair_df['PC_distance'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = all_df.copy()\n",
    "# convert back to EPSG 4326 for saving geographic coordinates\n",
    "geometry = geometry.to_crs(4326)\n",
    "\n",
    "all_df['midpoint_lat_deg_N'] = midpoint.apply(lambda mp: mp.y)\n",
    "all_df['midpoint_lon_deg_E'] = midpoint.apply(lambda mp: mp.x)\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_measure_COD(pair):\n",
    "    df1 = extract_streamflow_series(pair[0])\n",
    "#     df1.rename(mapper={'discharge': f'{pair[0]}'}, inplace=True, axis=1)\n",
    "    \n",
    "    df2 = extract_streamflow_series(pair[1])\n",
    "#     df2.rename(mapper={'discharge': f'{pair[1]}'}, inplace=True, axis=1)\n",
    "    concurrent_df = pd.concat([df1, df2], join='inner', axis=1)\n",
    "    if len(concurrent_df) >= 365:\n",
    "        cols = concurrent_df.columns\n",
    "        out = st.linregress(concurrent_df.to_numpy())    \n",
    "\n",
    "        return out[2]**2\n",
    "    else:\n",
    "        return np.nan\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_characteristics_cols\n",
    "for char in basin_characteristics_cols:\n",
    "#     b1_char = \n",
    "    all_df[f'{char}_diff'] = [hysets_dict[b1][char] - hysets_dict[b2][char] for b1, b2 in all_df[['b1', 'b2']].to_numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pool = Pool()\n",
    "t0 = time.time()\n",
    "all_df['similarity'] = pool.map(calculate_similarity_measure_COD, all_df[['b1', 'b2']].to_numpy())\n",
    "pool.close()\n",
    "pool.join()\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f't for {len(all_df)} results: {t1-t0:.1f}s')\n",
    "all_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df = all_df.copy()\n",
    "print(len(results_df))\n",
    "results_df.dropna(how='any', inplace=True)\n",
    "print(len(results_df))\n",
    "results_df.to_csv('results/results_min_365d_concurrent.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "foo = pd.read_csv('results/results_min_365d_concurrent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo[foo['similarity'].isna()].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
