{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA) of Basin Characteristics\n",
    "\n",
    "In estimating runoff in ungauged basins, candidate \"proxy\" basins are sought on the basis of similarity of the mechanisms governing runoff.  Intuitively, adjacent basins of similar size, shape, aspect, topology, land cover, geology, etc. are expected to generate similar runoff in volume and duration, if not timing.  Based on some measure of similarity, in this case daily average flow, are there basin characteristics that are consistently better predictors of similarity?  Are there combinations of characteristics, or principal components, that form a more meaningful basis to describe similarity?  In this example, patterns are sought in a large sample of watersheds in Canada featuring a common set of basin characteristics.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The basins included in the dataset are a subset of the HYSETS database with HYDAT as the source, as these represent basins operated by the Water Survey of Canada.  A later step of the analysis evaluates runoff and basin characteristics for pairs of basins with a minimum length of concurrent data. The HYSETS database contains 2375 basins, and of all ($5.6E6$) basin pair combinations, $2.3E6$ basin pairs meet a minimum of 40 years of concurrent daily flow data.  There are 1030 unique basins in the set of all basin pairs meeting the minimum concurrence.  Of the remaining basins, 759 have a complete set of 15 basin characteristics.  (include discussion about choice of minimum concurrence, provide dataset with varying concurrence threshold criteria.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "\n",
    "The dataset is constructed of $N$ rows corresponding to unique basins and $m$ columns corresponding basin characteristics, represented by the matrix $$\\textbf{Y} = \\begin{bmatrix}y_{11} & y_{12} & \\dots & y_{1m} \\\\ y_{21} & y_{22} & \\dots & y_{2m} \\\\ \\dots & \\dots & \\dots & \\dots \\\\ y_{N1} & y_{N2} & \\dots & y_{Nm} \\end{bmatrix}$$\n",
    "\n",
    "PCA is used to extract uncorrelated modes of variability in the data, on the basis that some more fundamental set of variables $e_1, e_2, \\dots$ exists and describes the data (potentially) more concisely.  Following the method of Hotelling (1933), the data is standardized such that each variable (column) has zero mean and unit variance.  Given $m$ variables and $N$ samples, the objective is to find a matrix $$\\textbf{e} = \\begin{bmatrix}e_{11} & e_{12} & \\dots & e_{1m} \\\\ e_{21} & e_{22} & \\dots & e_{2m} \\\\ \\dots & \\dots & \\dots & \\dots \\\\ e_{m1} & e_{m2} & \\dots & e_{mm} \\end{bmatrix}$$ such that $$\\textbf{Y} - \\mathbf{\\bar{Y}}  = \\sum_{j=1}^{m} \\textbf{a}_j \\textbf{e}_j, \\quad j = 1, 2, \\dots, m$$  \n",
    "\n",
    "The eigenvector columns $\\textbf{e}_j$ describe the rotated coordinate axes of the PC modes, while the $\\textbf{a}_j$ principal component \"scores\", or the \"signal\" of each variable, represented in the rotated space.  The modes describe the dynamics of the system in combinations of the original variables, and in descending order of fractional variance, that is the proportional amount of the total variance explained by the mode.  The eigenvectors by definition align with the axis yielding the greatest variance, however this does not necessarily mean the eigenvector will be aligned meaningfully with the data.  As a result, the PC scores ...\n",
    "\n",
    "Plot the squared sum of square PC score (differences between pairs) vs. similarity?  Do we expect this to be any different than the unrotated space?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from minisom import MiniSom\n",
    "import somoclu\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "import geoviews as gv\n",
    "from geoviews import opts, tile_sources as gvts\n",
    "\n",
    "import scipy.stats as st\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "\n",
    "from bokeh.plotting import figure, output_file, show, output_notebook\n",
    "from bokeh.plotting import ColumnDataSource\n",
    "from bokeh.transform import factor_cmap, factor_mark\n",
    "from bokeh.palettes import Spectral3\n",
    "from bokeh.layouts import gridplot, column, row\n",
    "from bokeh.io import output_file, save, show\n",
    "\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "from holoviews import dim\n",
    "hv.extension('bokeh')\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do PCA on Characteristics of Individual Basins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pair_results_df = pd.read_pickle('results/filtered_pairs_all_concurrent_lengths.csv')\n",
    "\n",
    "unique_stations = all_pair_results_df[['b1', 'b2']].to_numpy().flatten()\n",
    "\n",
    "unique_stations = list(set(unique_stations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_df = pd.read_csv('data/HYSETS_watershed_properties.txt', sep=';', dtype={'Official_ID': str})\n",
    "basin_df = basin_df[basin_df['Source'] == 'HYDAT']\n",
    "basin_df = basin_df[basin_df['Official_ID'].isin(unique_stations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_pca_cols = [ 'Centroid_Lat_deg_N', 'Centroid_Lon_deg_E', 'Drainage_Area_km2', 'Elevation_m','Slope_deg', \n",
    "                  'Gravelius', 'Perimeter', 'Aspect_deg', 'Land_Use_Forest_frac', 'Land_Use_Grass_frac', \n",
    "                  'Land_Use_Wetland_frac', 'Land_Use_Water_frac', 'Land_Use_Urban_frac', 'Land_Use_Shrubs_frac', \n",
    "                  'Land_Use_Crops_frac', 'Land_Use_Snow_Ice_frac', 'Permeability_logk_m2', 'Porosity_frac']\n",
    "\n",
    "# exclude any basins that are missing basin characteristics that will be used in \n",
    "basins_missing_data = basin_df[basin_df[basin_pca_cols].isnull().any(1)]['Official_ID'].to_numpy()\n",
    "\n",
    "basin_df = basin_df[~basin_df[basin_pca_cols].isnull().any(1)]\n",
    "\n",
    "basin_df.reset_index(inplace=True)\n",
    "basin_df['index'] = basin_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the two basins that are missing data.  Not sure how these got in here in the first place, but...\n",
    "print(len(all_pair_results_df))\n",
    "print(basins_missing_data)\n",
    "all_pair_results_df = all_pair_results_df[(~all_pair_results_df['b1'].isin(basins_missing_data)) & (~all_pair_results_df['b2'].isin(basins_missing_data))]\n",
    "print(len(all_pair_results_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict to map the Official ID to the index\n",
    "idx_id_dict = basin_df[['Official_ID', 'index']].set_index('Official_ID').to_dict()['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normed_df = (rdf - rdf.min()) / (rdf.max() - rdf.min())\n",
    "# normed_df.dropna(inplace=True)\n",
    "# print(f'Analyzing {len(raw_df)} basin pairs.')\n",
    "print(f'Analyzing {len(basin_df)} basins.')\n",
    "# print(basin_df.head())\n",
    "# filter for the basin characteristic columns\n",
    "basin_PCA_df = basin_df[basin_pca_cols].copy()\n",
    "\n",
    "# standardize the input data\n",
    "basin_PCA_df = (basin_PCA_df - basin_PCA_df.mean()) / basin_PCA_df.std()\n",
    "\n",
    "# check mean and stdev are 0 and 1, respectively\n",
    "# print(basin_PCA_df.mean())\n",
    "# print(basin_PCA_df.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = basin_PCA_df.plot(figsize=(12, 5))\n",
    "\n",
    "ax.set_xlabel('basin # [-]')\n",
    "ax.set_ylabel('value [-]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid_plot(data):\n",
    "    j = 0\n",
    "    figs = []\n",
    "    for char in data.columns:\n",
    "        p = figure(plot_width=250, plot_height=250, background_fill_color=\"#fafafa\",\n",
    "                  title=char)\n",
    "        x = data[char].to_numpy()\n",
    "        \n",
    "        hist, edges = np.histogram(x, density=True, bins='auto')\n",
    "        \n",
    "        \n",
    "        p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:],\n",
    "           fill_color=\"navy\", line_color=\"white\", alpha=0.5)\n",
    "        figs.append(p)\n",
    "    return figs\n",
    "            \n",
    "hist_grid = create_grid_plot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(gridplot(hist_grid, ncols=4, plot_width=200, plot_height=130))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do PCA \n",
    "x = basin_PCA_df.index.values\n",
    "y = basin_PCA_df.to_numpy()\n",
    "\n",
    "n_modes = np.shape(y)[1] #dimension of input = 4, so want 4 modes\n",
    "\n",
    "pca = PCA(n_components = n_modes)\n",
    "PCs = pca.fit_transform(y)\n",
    "eigvecs = pca.components_\n",
    "fracVar = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Input shape = ', np.shape(y))\n",
    "print('PC shape = ', np.shape(PCs))\n",
    "print('Eigenvector shape', np.shape(eigvecs))\n",
    "print('PCA Output Eigenvectors (first):')\n",
    "print(eigvecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort eigenvectors and add labels for the eigenvector plot\n",
    "def sort_eigenvectors(eigvecs, pca_variables):\n",
    "    rank_dict = {}\n",
    "    ev_num = 0\n",
    "    for ev in eigvecs:\n",
    "        sorted_evs_idx = ev.argsort()\n",
    "        sorted_evs = ev[sorted_evs_idx][::-1]\n",
    "        reordered_pca_vars = np.array(pca_variables)[sorted_evs_idx][::-1]\n",
    "        rank_dict[ev_num] = {k: v for k, v in zip(reordered_pca_vars, sorted_evs)}\n",
    "        ev_num += 1\n",
    "    return rank_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_dict =  sort_eigenvectors(eigvecs, basin_pca_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot fraction of variance explained by each mode\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(range(len(fracVar)),fracVar)\n",
    "plt.xlabel('Mode Number')\n",
    "plt.ylabel('Fraction Variance Explained')\n",
    "plt.title('Variance Explained by All Modes')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "n_modes_show = 4\n",
    "plt.scatter(range(n_modes_show),fracVar[:n_modes_show])\n",
    "plt.xlabel('Mode Number')\n",
    "plt.ylabel('Fraction Variance Explained')\n",
    "plt.title('Variance Explained by First ' + str(n_modes_show) + ' Modes')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_var = 0\n",
    "for n in range(len(fracVar)):\n",
    "    cumulative_var += fracVar[n]\n",
    "    print(f'Mode {n}: {fracVar[n]:.2f} (cumulative {cumulative_var:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plot the first n modes and PCs \n",
    "n = PCs.shape[1]\n",
    "plt.rcParams['savefig.facecolor'] = \"0.8\"\n",
    "plt.figure(figsize=(9,4*n))\n",
    "for kk in range(n):\n",
    "    \n",
    "    plt.subplot(n,2,kk*2+1)\n",
    "#     plt.plot(eigvecs[kk,:])\n",
    "    plt.plot(rank_dict[kk].keys(), rank_dict[kk].values())\n",
    "    plt.title(f'Eigenvector of Mode #{kk+1}')\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    plt.subplot(n,2,(kk+1)*2)\n",
    "    plt.plot(PCs[:,kk])\n",
    "    plt.title(f'PCs of Mode #{kk+1}')\n",
    "    plt.xlabel('Signal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "#     plt.savefig('results/img/ordered_eigvecs_characteristics.png', transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show pairs of variables in 2-d eigenspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_df = pd.DataFrame(PCs, columns=[f'PC {e}' for e in range(PCs.shape[1])])\n",
    "pca_component_grid = create_grid_plot(pc_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(gridplot(pca_component_grid, ncols=4, plot_width=200, plot_height=130))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_PC_gridplot(PCs, n_modes):\n",
    "    mode_pairs = list(combinations(list(range(n_modes)), 2))\n",
    "    figs = []\n",
    "    for pair in mode_pairs:\n",
    "        \n",
    "\n",
    "        p = figure(plot_width=250, plot_height=250, background_fill_color=\"#fafafa\",\n",
    "                  title=f'{pair}')\n",
    "\n",
    "        x1 = PCs[:, pair[0]]\n",
    "        x2 = PCs[:, pair[1]]\n",
    "\n",
    "        p.circle(x1, x2, alpha=0.5)\n",
    "        figs.append(p)\n",
    "    return figs\n",
    "    \n",
    "    \n",
    "pc_space_figs = make_PC_gridplot(PCs, 9)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(gridplot(pc_space_figs, ncols=6, plot_width=150, plot_height=130))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate euclidean distance of PC scores for basin pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hysets_folder = '/media/danbot/T7 Touch/hysets_series/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_band(rdf, param, n_bins):\n",
    "       \n",
    "    if param == 'similarity':\n",
    "        domain = 'normed_distance'\n",
    "    else:\n",
    "        domain = 'similarity'\n",
    "        \n",
    "#     hist, bin_edges = np.histogram(rdf[domain], density=True, bin_edges=b_edges)\n",
    "#     n_samples_per_bin = 200\n",
    "#     n_bins_100_sample = int(len(rdf) / n_samples_per_bin)\n",
    "#     b_edges1 = st.mstats.mquantiles(rdf[param], np.linspace(0., 1.0, n_bins_100_sample))\n",
    "    b_edges = st.mstats.mquantiles(rdf[param], np.linspace(0., 1.0, n_bins))\n",
    "#     print(b_edges)\n",
    "    b_edges = np.where(np.isnan(b_edges), 1, b_edges)\n",
    "#     print(b_edges)\n",
    "    \n",
    "    tdf = pd.DataFrame()\n",
    "    tdf['bin_edges'] = b_edges\n",
    "\n",
    "    median_vals, lbound1, hbound1 = [], [], []\n",
    "    bin_vals, lbound2, hbound2 = [], [], []\n",
    "\n",
    "    for i in range(1,len(tdf)):\n",
    "        min_edge = tdf.loc[i-1, 'bin_edges']\n",
    "        max_edge = tdf.loc[i, 'bin_edges']\n",
    "        mid_bin = (min_edge + max_edge) / 2\n",
    "        bin_vals.append(mid_bin)\n",
    "        grouped_vals = rdf[(rdf[param] >= min_edge) & (rdf[param] < max_edge)][domain]\n",
    "#         print(f'{len(grouped_vals)} values between {min_edge:.5f} and {max_edge:.5f}')\n",
    "        \n",
    "        if len(grouped_vals) < 1:\n",
    "            median_vals.append(np.nan)\n",
    "            lbound1.append(np.nan)\n",
    "            hbound1.append(np.nan)\n",
    "            lbound2.append(np.nan)\n",
    "            hbound2.append(np.nan)\n",
    "\n",
    "        else:\n",
    "            median_vals.append(np.percentile(grouped_vals, 50))\n",
    "            lbound1.append(np.percentile(grouped_vals, 5))\n",
    "            hbound1.append(np.percentile(grouped_vals, 95))\n",
    "            lbound2.append(np.percentile(grouped_vals, 33.33333))\n",
    "            hbound2.append(np.percentile(grouped_vals, 66.666667))\n",
    "            \n",
    "            \n",
    "    band = pd.DataFrame()\n",
    "    band['edge'] = bin_vals\n",
    "\n",
    "    band['median'] = median_vals\n",
    "    band['lbnd1'] = lbound1\n",
    "    band['hbnd1'] = hbound1\n",
    "\n",
    "    band['lbnd2'] = lbound2\n",
    "    band['hbnd2'] = hbound2\n",
    "#     print('')\n",
    "  \n",
    "\n",
    "\n",
    "    return band\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " #output to static HTML file\n",
    "def create_fig(band, rdf, param):\n",
    "    \n",
    "    title = f\"Expected similarity metric by equiprobable distance metric binning ({len(rdf)} basin pairs)\"\n",
    "    if param == 'similarity':\n",
    "        title = f\"Expected distance metric by equiprobable similarity metric binning ({len(rdf)} basin pairs)\"\n",
    "\n",
    "    p = figure(plot_width=600, plot_height=550, tools=\"wheel_zoom,box_zoom,pan,reset,lasso_select\",\n",
    "               title=title,\n",
    "              match_aspect=True, background_fill_color='#440154')\n",
    "\n",
    "    p.grid.visible = False\n",
    "    \n",
    "#     bins = hexbin(rdf['normed_dist'], rdf['similarity'], 0.05)\n",
    "#     print(bins)\n",
    "    \n",
    "#     cmap = linear_cmap('counts', 'Viridis256', 0, max(bins.counts))\n",
    "\n",
    "#     h = p.hex_tile(q=\"q\", r=\"r\", size=0.01, line_color=None, source=bins,\n",
    "#                fill_color=cmap)\n",
    "    \n",
    "    p.circle(rdf['normed_distance'], rdf['similarity'], color=\"white\", size=1,\n",
    "            alpha=0.6)\n",
    "    \n",
    "    rdf.dropna(inplace=True)\n",
    "    \n",
    "    if param == 'similarity':\n",
    "\n",
    "        domain = 'normed_distance'\n",
    "        p.harea(band['lbnd1'], band['hbnd1'], \n",
    "                 band['edge'], alpha=0.3, color='white')\n",
    "        p.harea(band['lbnd2'], band['hbnd2'], \n",
    "                 band['edge'], alpha=0.5, color='white')\n",
    "        \n",
    "        sim_all = np.percentile(rdf[domain], 50)\n",
    "        sim_lb1 = np.percentile(rdf[domain], 5)\n",
    "        sim_lb2 = np.percentile(rdf[domain], 33.33333)\n",
    "        sim_hb1 = np.percentile(rdf[domain], 95)\n",
    "        sim_hb1 = np.percentile(rdf[domain], 66.66667)\n",
    "        \n",
    "        y_vals = np.linspace(0, 1, 20)\n",
    "        x_vals = [sim_all for _ in y_vals]\n",
    "#         print(sim_all)\n",
    "        p.line(x_vals, y_vals, color='#F012BE', line_width=3, \n",
    "              legend_label='EV (all data)', line_dash='dashed')\n",
    "        \n",
    "        p.line(band['median'], band['edge'], color='#F012BE', \n",
    "               line_width=2, legend_label='EV (bin)')\n",
    "        \n",
    "    else:\n",
    "\n",
    "        domain = 'similarity'\n",
    "        p.varea(band['edge'], band['lbnd1'], band['hbnd1'], \n",
    "                 alpha=0.3, color='white')\n",
    "        p.varea(band['edge'], band['lbnd2'], band['hbnd2'], \n",
    "                 alpha=0.5, color='white')\n",
    "        \n",
    "        similarity_all = np.percentile(rdf['normed_distance'], 50)\n",
    "        \n",
    "\n",
    "        sim_all = np.percentile(rdf[domain], 50)\n",
    "        sim_lb1 = np.percentile(rdf[domain], 5)\n",
    "        sim_lb2 = np.percentile(rdf[domain], 33.33333)\n",
    "        sim_hb1 = np.percentile(rdf[domain], 95)\n",
    "        sim_hb1 = np.percentile(rdf[domain], 66.66667)\n",
    "    \n",
    "        x_vals = np.linspace(0, 1, 20)\n",
    "        y_vals = [sim_all for _ in x_vals]\n",
    "        \n",
    "\n",
    "        \n",
    "        p.line(x_vals, y_vals, color='#F012BE', line_width=3,\n",
    "              legend_label='EV (all data)', line_dash='dashed')\n",
    "        \n",
    "        p.line(band['edge'], band['median'], color='#F012BE',\n",
    "              line_width=2, legend_label='EV (bin)')\n",
    "\n",
    "#     hover = HoverTool(\n",
    "#         tooltips=[('distance', '@normed_distance'),\n",
    "#                  ()],\n",
    "#         mode='mouse',\n",
    "#         point_policy='follow_mouse',\n",
    "#         renderers=[h],\n",
    "#     )\n",
    "#     p.add_tools(hover)\n",
    "        \n",
    "#     min_d = rdf['normed_distance'].min()\n",
    "#     max_d = rdf['normed_distance'].max()\n",
    "\n",
    "    p.xaxis.axis_label = f'Euclidean Distance of Parameter Differences (1 = most dissimilar)'\n",
    "    p.yaxis.axis_label = f'Similarity Metric (COD/R^2)'\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_layout(normed_df, n_bins, modes):\n",
    "    similarity_df = create_band(normed_df, 'similarity', n_bins).dropna()\n",
    "    dist_df = create_band(normed_df, 'normed_distance', n_bins).dropna()\n",
    "    p1 = create_fig(similarity_df, normed_df, 'similarity')\n",
    "    p2 = create_fig(dist_df, normed_df, 'normed_distance')\n",
    "#     print(dist_df.head())\n",
    "    \n",
    "#     return (dist_df['edge'].to_numpy(), dist_df['median'].to_numpy())\n",
    "    return row(p1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_bins  = 50\n",
    "\n",
    "mode = 0 # PC mode\n",
    "\n",
    "# band = create_band(pair_df, 'normed_distance', n_bins).dropna()\n",
    "\n",
    "# layout = create_layout(pair_df, 50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the PC scores in Space\n",
    "\n",
    "**NOTE:** this is for the station PCs only, not basin pair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spdf = pc_df.copy()\n",
    "# spdf.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "spdf['geometry'] = basin_df.apply(lambda row: Point(row['Centroid_Lon_deg_E'],\trow['Centroid_Lat_deg_N']), axis=1)\n",
    "# spdf['lon'] = basin_df['Centroid_Lon_deg_E']\n",
    "# spdf['lat'] = basin_df['Centroid_Lat_deg_N']\n",
    "# gdf = \n",
    "# spdf.head()\n",
    "\n",
    "gdf = gpd.GeoDataFrame(spdf, crs='EPSG:4326')\n",
    "gdf = gdf.to_crs(3857)\n",
    "# gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_PC_dist(row, modes):\n",
    "    cols = [f'PC {m}' for m in modes]\n",
    "    return np.sqrt((row[cols]**2).sum())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modes_of_interest = [0]\n",
    "\n",
    "gdf['stn_ID'] = basin_df['Official_ID']\n",
    "gdf['PC_distance'] = gdf.apply(lambda row: calculate_PC_dist(row, modes_of_interest), axis=1)\n",
    "\n",
    "\n",
    "# retrieve the centroid lat-lon and convert to shapely Point\n",
    "points = hv.Points(gdf, vdims=['PC_distance', 'stn_ID'])\n",
    "\n",
    "# x_range = (spdf.min()['lon'], spdf.max()['lon'])\n",
    "# y_range = (spdf.min()['lat'], spdf.max()['lat'])\n",
    "\n",
    "# tl = hv.element.tiles.StamenTerrain().redim.range()\n",
    "\n",
    "gvts.EsriTerrain.opts(width=900, height=550) * points.opts(color='PC_distance', tools=['hover'], \n",
    "                                                           size=4, colorbar=True, \n",
    "                                                           title=f'PC score (mode set {modes_of_interest})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now do PCA for station pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = pd.read_csv('results/results_min_365d_concurrent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(cdf['pair_midpoint'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_pca_df = cdf.copy()\n",
    "pair_pca_chars = [c for c in cdf.columns if c not in ['similarity', 'b1', 'b2', 'concurrent_days', 'pair_midpoint']]\n",
    "print(char_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_PCA(df, char_cols):\n",
    "        \n",
    "    # standardize to zero mean and unit stdev (but exclude similarity column)\n",
    "    df.loc[:,char_cols] = (df.loc[:, char_cols] - df.loc[:, char_cols].mean()) / df.loc[:, char_cols].std()\n",
    "\n",
    "    # do PCA \n",
    "    x = df.index.values\n",
    "    y = df[char_cols].to_numpy()\n",
    "\n",
    "    n_modes = np.shape(y)[1] #full dimension of input\n",
    "\n",
    "    pca = PCA(n_components = n_modes)\n",
    "    PCs = pca.fit_transform(y)\n",
    "    eigvecs = pca.components_\n",
    "    fracVar = pca.explained_variance_ratio_\n",
    "    return PCs, eigvecs, fracVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_PCs, pair_eigvecs, pair_fracVar = do_PCA(cdf, pair_pca_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_cumulative_var = 0\n",
    "for n in range(len(pair_fracVar)):\n",
    "    pair_cumulative_var += pair_fracVar[n]\n",
    "    print(f'Mode {n}: {pair_fracVar[n]:.2f} (cumulative {pair_cumulative_var:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot fraction of variance explained by each mode\n",
    "n_modes_show = pair_PCs.shape[1]\n",
    "# pair_eigvec_dict = sort_eigenvectors(pair_eigvecs)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(range(len(pair_fracVar)),pair_fracVar)\n",
    "plt.xlabel('Mode Number')\n",
    "plt.ylabel('Fraction Variance Explained')\n",
    "plt.title('Variance Explained by All Modes')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "n_modes_show = 4\n",
    "plt.scatter(range(n_modes_show),pair_fracVar[:n_modes_show])\n",
    "plt.xlabel('Mode Number')\n",
    "plt.ylabel('Fraction Variance Explained')\n",
    "plt.title('Variance Explained by First ' + str(n_modes_show) + ' Modes')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plot the first n modes and PCs for ~90% of variance\n",
    "n = pair_PCs.shape[1]\n",
    "pair_rank_dict = sort_eigenvectors(pair_eigvecs, pair_pca_chars)\n",
    "# plt.rcParams['savefig.facecolor'] = \"0.8\"\n",
    "plt.figure(figsize=(9,4*n))\n",
    "for kk in range(n):\n",
    "    \n",
    "    plt.subplot(n,2,kk*2+1)\n",
    "#     plt.plot(eigvecs[kk,:])\n",
    "    plt.plot(pair_rank_dict[kk].keys(), pair_rank_dict[kk].values())\n",
    "    plt.title(f'Eigenvector of Mode #{kk}')\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    plt.subplot(n,2,(kk+1)*2)\n",
    "    plt.plot(pair_PCs[:,kk])\n",
    "    plt.title(f'PCs of Mode #{kk}')\n",
    "    plt.xlabel('Signal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/img/ordered_eigvecs_PCscores.png', transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_pc_df = pd.DataFrame(pair_PCs, columns=[f'PC {i}' for i in range(pair_PCs.shape[1])])\n",
    "pair_pc_df['similarity'] = cdf['similarity'].copy()\n",
    "pair_pc_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_band(data, n_bins):\n",
    "        # Use alpha, beta = 0.33, 0.33 according to\n",
    "    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mstats.mquantiles.html\n",
    "    b_edges = st.mstats.mquantiles(data['PC_distance_norm'], np.linspace(0., 1.0, n_bins),\n",
    "                                  alphap=1./3, betap=1./3)\n",
    "#     print(b_edges)\n",
    "    b_edges = np.where(np.isnan(b_edges), 1, b_edges)\n",
    "#     print(b_edges)\n",
    "    \n",
    "    tdf = pd.DataFrame()\n",
    "    tdf['bin_edges'] = b_edges\n",
    "\n",
    "    median_vals, lbound1, hbound1 = [], [], []\n",
    "    bin_vals, lbound2, hbound2 = [], [], []\n",
    "\n",
    "    for i in range(1,len(tdf)):\n",
    "        min_edge = tdf.loc[i-1, 'bin_edges']\n",
    "        max_edge = tdf.loc[i, 'bin_edges']\n",
    "        mid_bin = (min_edge + max_edge) / 2\n",
    "        bin_vals.append(mid_bin)\n",
    "        grouped_vals = data[(data['PC_distance_norm'] >= min_edge) & (data['PC_distance_norm'] < max_edge)]['similarity']\n",
    "#         print(f'{len(grouped_vals)} values between {min_edge:.5f} and {max_edge:.5f}')\n",
    "        \n",
    "        if len(grouped_vals) < 1:\n",
    "            median_vals.append(np.nan)\n",
    "            lbound1.append(np.nan)\n",
    "            hbound1.append(np.nan)\n",
    "            lbound2.append(np.nan)\n",
    "            hbound2.append(np.nan)\n",
    "\n",
    "        else:\n",
    "            median_vals.append(np.percentile(grouped_vals, 50))\n",
    "            lbound1.append(np.percentile(grouped_vals, 5))\n",
    "            hbound1.append(np.percentile(grouped_vals, 95))\n",
    "            lbound2.append(np.percentile(grouped_vals, 33.33333))\n",
    "            hbound2.append(np.percentile(grouped_vals, 66.666667))\n",
    "            \n",
    "            \n",
    "    band = pd.DataFrame()\n",
    "    band['edge'] = bin_vals\n",
    "\n",
    "    band['median'] = median_vals\n",
    "    band['lbnd1'] = lbound1\n",
    "    band['hbnd1'] = hbound1\n",
    "\n",
    "    band['lbnd2'] = lbound2\n",
    "    band['hbnd2'] = hbound2\n",
    "    \n",
    "    return band\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pair_pc_df(pair_pc_df, modes_of_interest):\n",
    "    \n",
    "    df = pair_pc_df.copy()\n",
    "    \n",
    "    mode_cols = [f'PC {m}' for m in modes_of_interest]\n",
    "\n",
    "    df['PC_distance'] = np.sqrt((df[mode_cols]**2).sum(1))\n",
    "\n",
    "    df['PC_distance_norm'] = (df['PC_distance'] - df['PC_distance'].min()) / (df['PC_distance'].max() - df['PC_distance'].min())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_pc_df = update_pair_pc_df(pair_pc_df, list(range(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_df = create_band(pair_pc_df[['PC_distance_norm', 'similarity']], 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_line = hv.HLine(np.percentile(pair_pc_df['similarity'], 50))\n",
    "binned_line = hv.Curve(band_df, ['edge', 'median'])\n",
    "hex_two_distributions = hv.HexTiles(pair_pc_df[['PC_distance_norm', 'similarity']].to_numpy())\n",
    "hex_two_distributions.opts(scale=(dim('Count').norm()*0.5)+0.3, width=500, height=500)\n",
    "\n",
    "overlay = hex_two_distributions * hv.Bivariate(hex_two_distributions) * ev_line * binned_line\n",
    "\n",
    "overlay.opts(\n",
    "    opts.HexTiles(min_count=0, width=650, height=500, tools=['hover'], \n",
    "                  title=f'{len(pair_pc_df)} pairs, min 365 d. concurrent, Modes {modes_of_interest}',\n",
    "                  xlabel='Euclidean Distance of PC Scores', ylabel='Similarity [R^2]'),\n",
    "    opts.Bivariate(show_legend=False, line_width=3),\n",
    "    opts.HLine(color='red', line_width=3, line_dash='dashed'),\n",
    "    opts.Curve(color='red', line_width=3, line_dash='solid')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Many Combinations of Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe_by_concurrence(cdf, min_concurrence_len=None, max_concurrence_len=None):\n",
    "    df = cdf.copy()\n",
    "    \n",
    "    if min_concurrence_len is not None:\n",
    "        df = df[df['concurrent_days'] > 364 * min_concurrence_len]\n",
    "    if max_concurrence_len is not None:\n",
    "        df = df[df['concurrent_days'] <= 365 * max_concurrence_len]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode_combos = [[0], [1], [2], [3], [4], [5], [6],\n",
    "#                [0,1], [0,2], [0,3], [0,4], [0,5], [0,6],\n",
    "#                [1,2], [1,3], [1,4], [1,5], [1,6],\n",
    "#                [2,3], [2,4], [2,5], [2,6],\n",
    "#                [3,4], [3,5], [3,6],\n",
    "#                [4,5], [4,6],\n",
    "#                [5,6],\n",
    "#                [0,1,2], [0,1,3], [0,2,3], [0,2,4], [0,3,4], [0,1,4],\n",
    "#                [1,2,3], [1,2,4], [1,3,4], [1,3,5], [1,2,5],\n",
    "#                [0,1,2,3], [0,2,3,4], [1,2,3,4], [0,3,4,5],\n",
    "#                [0,1,2,3,4], [0,1,2,3,5], [0,1,2,4,5], [0,1,3,4,5], \n",
    "#                [0,1,2,3,4,5], [0,1,2,3,5,6], [0,1,2,4,5,6], [0,1,3,4,5,6], \n",
    "#               ]\n",
    "\n",
    "mode_combos = [\n",
    "    [0], [1], [2], [3], [4], [5], [6], [7], [8], [9],\n",
    "              [0,1], [1,2], [2,3], [3,4], [4,5], [5,6], [6,7], [7,8], [8,9],\n",
    "              [0,1,2], [1,2,3], [2,3,4], [3,4,5], [4,5,6], [5,6,7], [6,7,8], [7,8,9],\n",
    "              [0,1,2,3], [1,2,3,4], [2,3,4,5], [3,4,5,6], [4,5,6,7], [5,6,7,8], [6,7,8,9],\n",
    "              [0,1,2,3,4], [1,2,3,4,5], [2,3,4,5,6], [3,4,5,6,7], [4,5,6,7,8], [5,6,7,8,9],\n",
    "              [0,1,2,3,4,5], [1,2,3,4,5,6], [2,3,4,5,6,7], [3,4,5,6,7,8], [4,5,6,7,8,9],\n",
    "              [0,1,2,3,4,5,6], [1,2,3,4,5,6,7], [2,3,4,5,6,7,8], [3,4,5,6,7,8,9],\n",
    "              [0,1,2,3,4,5,6,7], [1,2,3,4,5,6,7,8], [2,3,4,5,6,7,8,9],\n",
    "              [0,1,2,3,4,5,6,7,8], [1,2,3,4,5,6,7,8,9],\n",
    "              [0,1,2,3,4,5,6,7,8,9,10],\n",
    "              ]\n",
    "\n",
    "combo_result_dict, lengths_dict = {}, {}\n",
    "\n",
    "# set the range of concurrent periods to be evaluated\n",
    "min_conc, max_conc = 50, None\n",
    "\n",
    "data = filter_dataframe_by_concurrence(cdf, min_conc, max_conc)\n",
    "median_similarity = data['similarity'].median()\n",
    "\n",
    "for c in mode_combos:\n",
    "    \n",
    "    PCs, eigvecs, fracVar = do_PCA(data, char_cols)\n",
    "    \n",
    "#     print(PCs.shape, len(data))\n",
    "    \n",
    "    pc_data = pd.DataFrame(PCs, columns=[f'PC {i}' for i in range(PCs.shape[1])])\n",
    "#     print(data[data['similarity'].isna()].count())\n",
    "    pc_df_updated = update_pair_pc_df(pc_data, c)\n",
    "    \n",
    "#     print(data['similarity'].isna().sum())\n",
    "#     print(len(data), len(pc_df))\n",
    "    \n",
    "    pc_df_updated['similarity'] = list(data['similarity'].to_numpy())\n",
    "    \n",
    "    lengths_dict[str(c)] = {'n_pairs': len(pc_df_updated),\n",
    "                            'min_max_conc': (min_conc, max_conc)}\n",
    "\n",
    "    band = create_band(pc_df_updated, 25)\n",
    "    combo_result_dict[str(c)] = band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_out_df = pd.DataFrame()\n",
    "for c in combo_result_dict:\n",
    "    if multi_out_df.empty:\n",
    "        multi_out_df[f'edge'] = combo_result_dict[c]['edge'].copy()\n",
    "    multi_out_df[f'median_{c}'] = combo_result_dict[c]['median'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for k, df in combo_result_dict.items():\n",
    "#     print(df[['edge', 'median']].head())\n",
    "# combo_result_dict['[0]'].loc[0, 'median']\n",
    "# unique_mode_lengths = list(set([len(e) for e in combo_result_dict.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull together all the lines\n",
    "line_list = [hv.Path(combo_result_dict[k][['edge', f'median']]) for k, df in combo_result_dict.items()]\n",
    "\n",
    "# label the lines\n",
    "label_coords_left = [(-0.0, d.loc[0, 'median']) for k, d in combo_result_dict.items()]\n",
    "label_coords_right = [(d.loc[len(d)-1, ['edge', 'median']].to_numpy()) for k, d in combo_result_dict.items()]\n",
    "\n",
    "labels = [e[1:-1] for e in list(combo_result_dict.keys())]\n",
    "\n",
    "line_labels_left = hv.Labels({('x', 'y'): label_coords_left, 'text': labels}, ['x', 'y'], 'text')\n",
    "line_labels_right = hv.Labels({('x', 'y'): label_coords_right, 'text': labels}, ['x', 'y'], 'text')\n",
    "\n",
    "title_info = lengths_dict[list(lengths_dict.keys())[0]]\n",
    "n_pairs = title_info['n_pairs']\n",
    "min_dur = title_info['min_max_conc'][0]\n",
    "max_dur = title_info['min_max_conc'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ev_line = hv.HLine(median_similarity)\n",
    "\n",
    "overlay = line_labels_left * hv.Overlay(line_list) * line_labels_right * ev_line\n",
    "\n",
    "overlay.opts(\n",
    "    opts.Labels(text_font_size='8pt'),\n",
    "    opts.Overlay(width=1200, height=900,\n",
    "              xlabel='Euclidean Distance of PC Scores', \n",
    "              ylabel='Similarity [R^2]',\n",
    "              title=f'{n_pairs} pairs, {min_dur} < Concurrent Period Duration <= {max_dur}',\n",
    "             ),\n",
    "    opts.HLine(line_width=4, line_dash='dashed', color='firebrick')\n",
    ")\n",
    "# renderer = hv.renderer('bokeh')\n",
    "# renderer.save(overlay, 'results/img/EV_similarity_vs_PC_diff_distance_curves')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Describe the Coefficient of Determination\n",
    "\n",
    "The quality of a numerical model is described by how well its predictions correlate with observation.  In hydrology, the coefficient of determination (COD) is often used to describe how well synchronized is the timing and volume of flow in two different rivers.  The COD indicating perfect correlation is one, and zero represents total randomness.  In between is a continuum of values that don't mean much on their own but begin to express something when gathered in large numbers.\n",
    "\n",
    "But what exactly does a COD of 0.74 mean?  Why would an author include the extra 0.04 precision when the decision associated with its evaluation is probably closer to binary?  *Yes, this correlation is good enough to use to base my design input on; No, some other method or source of information is required in order to produce a useful value*.  What is the lowest value a scientist or engineer is willing to tolerate, to publish, or to use to form the basis of some critical design component?  The answer is of course relative to the question being asked of the data.  Relationships between sets of observations are described as *strong* or *weak*, they are *highly*, *fairly well* or *not well* correlated.  If you want to get creative, [there is a website](https://describingwords.io/for/correlation) with *329+ adjectives for correlation, ordered by usage frequency*.  @Moed_2017 offers the sound advice to \"*never take a correlation coefficient for granted and never consider merely the numerical outcome.*\"\n",
    "\n",
    "A particularly routine exercise for practitioners in the resource industry is the daily average flow correlation.  Characterization of runoff is often required in ungauged basins.  Long-term records are sought from nearby gauging stations to find the most representative proxy for runoff at the location of interest.  Qualitative comparisons are written to justify choosing one candidate proxy over another.  The comparisons are based on proximity, basin characteristics such as elevation, topography, geology, land use, etc.  Quantitative justification for a proxy is uncommon, because quantitative measures of basin characteristics are often unailable or are otherwise difficult to determine.  \n",
    "\n",
    "It has been shown **(citation)** that some basin characteristics are more important than others as far as indicating the strength of the relationship of runoff between two different basins.  \n",
    "\n",
    "The figures above describe a relationship between basin characteristics and the COD.  Basin characteristics are compared by a distance measure defined by the euclidean distance of the principal component scores over the number of modes describing **X%** of the variance in the data.  The figure above-left suggests that for a basin pairs to have a COD greater than 0.8, the distance measure is beyond 2 standard deviations from the expected value of the entire sample, where the sample is made up of COD values of all possible pairs of station records that have a minimum 40 years of concurrent record.  The expected value is then a baseline of a random selection from all possible proxy basins.\n",
    "\n",
    "The figure above-right describes the situation where the basin characteristics are known but one or both rivers are ungauged.  The figure suggests that only the smallest of distance measure falls outside of 2 standard deviations from the expected value of all comparisons.  Based on basin characteristics alone, if the distance measure (standard or principal component score) is greater than roughly 0.2, then a random selection of two basins is expected to produce a better correlation.  The distance measure of approximately 0.2 defines a baseline bound of random pair selection.  Note that a random pairing (doesn't appear to be) the same as a random selection of a proxy.  \n",
    "\n",
    "Can a tool be devised to take as input a set of basin characteristics and output the distribution of COD given all available data?  \n",
    "\n",
    "-how does the expected value change as a function of record length (i.e. here 40 years is used, but what if analysis is undertaken by concurrence length ranges?  i.e. 1 year, 2, 3, 5, 10, 20, 40 years?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ICA\n",
    "\n",
    "Find the modes of maximum statistical independence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do ICA using built-in library\n",
    "x = df.index.values\n",
    "y = df.to_numpy()\n",
    "\n",
    "n_modes = np.shape(y)[1] #dimension of input = 4, so want 4 modes\n",
    "\n",
    "ica = FastICA(n_components=n_modes)\n",
    "S_ = ica.fit_transform(y)  # Reconstruct signals\n",
    "A_ = ica.mixing_  # Get estimated mixing matrix\n",
    "\n",
    "# We can `prove` that the ICA model applies by reverting the unmixing.\n",
    "assert np.allclose(y, np.dot(S_, A_.T) + ica.mean_)\n",
    "\n",
    "# ica = FastICA()\n",
    "# S_ica_ = ica.fit(y).transform(y)\n",
    "S_ /= S_.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica_df = pd.DataFrame(S_, columns=[f'IC {e}' for e in range(PCs.shape[1])])\n",
    "ica_component_grid = create_grid_plot(ica_df)\n",
    "\n",
    "show(gridplot(ica_component_grid, ncols=4, plot_width=200, plot_height=130))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica_space_figs = make_PC_gridplot(S_, 9)\n",
    "show(gridplot(ica_space_figs, ncols=6, plot_width=200, plot_height=130))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run SOM -- this code creates/trains the SOM and calculates stats of interest\n",
    "\n",
    "nx = 3\n",
    "ny = 3\n",
    "\n",
    "d = y\n",
    "\n",
    "\n",
    "\n",
    "#make, initialize, and train the SOM\n",
    "print(d.shape)\n",
    "som = MiniSom(nx, ny, data.shape[1], sigma=1, learning_rate=0.5) # initialization of (ny x nx) SOM\n",
    "som.pca_weights_init(data)\n",
    "som.train_random(data, 100) # trains the SOM with 100 iterations\n",
    "\n",
    "qnt = som.quantization(data) #this is the pattern of the BMU of each observation (ie: has same size as data input to SOM)\n",
    "bmu_patterns = som.get_weights() #this is the pattern of each BMU; size = (nx, ny, len(data[0]))\n",
    "QE = som.quantization_error(data) #quantization error of map\n",
    "TE = som.topographic_error(data) #topographic error of map\n",
    "\n",
    "#calculate the BMU of each observation\n",
    "bmus = []\n",
    "for kk in range(len(data)):\n",
    "    bmus.append(som.winner(data[kk]))\n",
    "    \n",
    "#inds gives the sequential coordinates of each SOM node (useful for plotting)\n",
    "inds = []\n",
    "for ii in range(ny):\n",
    "    for jj in range(nx):\n",
    "        inds.append((ii,jj))\n",
    "\n",
    "# compute the frequency of each BMU\n",
    "freq = np.zeros((nx,ny))\n",
    "for bmu in bmus:\n",
    "    freq[bmu[0]][bmu[1]]+=1\n",
    "freq/=len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize\n",
    "from matplotlib import pylab\n",
    "\n",
    "fig, ax = plt.subplots(ny, nx, figsize=(2*nx,3*ny))\n",
    "row, col = 0, 0\n",
    "for kk in range(nx*ny):   \n",
    "    indx = inds[kk][1]\n",
    "    indy = inds[kk][0]\n",
    "#     plt.imshow(np.reshape(bmu_patterns[indx][indy],(faceH,faceW)).T,cmap='gray')\n",
    "    ax[col, row].plot(np.reshape(bmu_patterns[indx][indy], (17)).T)\n",
    "    ax[col, row].set_title('Freq = ' + str(freq[indx][indy]*100)[:4] + '%')\n",
    "\n",
    "#     ax[col, row].set_ylabel('Precipitation (Normalized)')\n",
    "#     ax[col, row].set_xlabel('Percentage of Basin Area')\n",
    "    col += 1\n",
    "    if col == ny:\n",
    "        col = 0\n",
    "        row += 1\n",
    "    \n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, loop through and create a range of sizes of SOMs and compare TE and QE \n",
    "\n",
    "ny_array = [2,2,3,4,5,4,5,5,6,7,8, 9, 10, 11]\n",
    "nx_array = [2,3,3,3,3,4,4,5,6,7,8, 9, 10, 11]\n",
    "\n",
    "QE = []\n",
    "TE = []\n",
    "for kk in range(len(ny_array)):\n",
    "    nx = nx_array[kk]\n",
    "    ny = ny_array[kk]\n",
    "    \n",
    "    #make, initialize, and train the SOM\n",
    "    data = df.to_numpy()\n",
    "    som = MiniSom(nx, ny, 17, sigma=0.3, learning_rate=0.4) # initialization of SOM\n",
    "    som.pca_weights_init(data)\n",
    "    som.train_random(data, 100) # trains the SOM with 100 iterations\n",
    "\n",
    "    qnt = som.quantization(data) #this is the pattern of the BMU of each observation (ie: has same size as data input to SOM)\n",
    "    bmu_patterns = som.get_weights() #this is the pattern of each BMU; size = (nx, ny, len(data[0]))\n",
    "    QE.append(som.quantization_error(data)) #quantization error of map\n",
    "    TE.append(som.topographic_error(data)) #topographic error of map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize QE and TE (two different y axes)\n",
    "\n",
    "fig, ax1= plt.subplots()\n",
    "\n",
    "#QE\n",
    "color = 'tab:red'\n",
    "ax1.set_ylabel('QE',color=color)\n",
    "ax1.plot(QE,color=color)\n",
    "ax1.tick_params(axis='y',labelcolor=color)\n",
    "\n",
    "#TE\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('TE',color=color)\n",
    "ax2.plot(TE,color=color)\n",
    "ax2.tick_params(axis='y',labelcolor=color)\n",
    "\n",
    "plt.title('QE and TE for Different Map Sizes')\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx, ny = 8, 8\n",
    "\n",
    "som = somoclu.Somoclu(nx, ny, compactsupport=False)\n",
    "%time som.train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som.view_component_planes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"red\"] * len(data)\n",
    "colors.extend([\"green\"] * len(data))\n",
    "colors.extend([\"blue\"] * len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som.view_umatrix(bestmatches=True, bestmatchcolors=colors, labels=list(range(len(data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som = somoclu.Somoclu(nx, ny, maptype=\"toroid\",\n",
    "                      compactsupport=False)\n",
    "som.train(data)\n",
    "som.view_umatrix(bestmatches=True, bestmatchcolors=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som = somoclu.Somoclu(nx, ny, gridtype=\"hexagonal\",\n",
    "                      compactsupport=False)\n",
    "som.train(data)\n",
    "som.view_umatrix(bestmatches=True, bestmatchcolors=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som = somoclu.Somoclu(nx, ny, maptype=\"toroid\",\n",
    "                      compactsupport=False, initialization=\"pca\")\n",
    "som.train(data)\n",
    "som.view_umatrix(bestmatches=True, bestmatchcolors=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som.cluster()\n",
    "som.view_umatrix(bestmatches=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
